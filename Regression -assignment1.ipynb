{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ead279-b81d-4ac8-b3b3-fefda4c9228b",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a26b0c-b43c-45b6-a53a-876a1c94c0c9",
   "metadata": {},
   "source": [
    "1. Linear REgression - \n",
    "\n",
    "Linear Regression is a statistical method used to findout the relationship between two continuous variable , in which \n",
    "typically one is target variable and one is predictor variable\n",
    "Linear regression used where only one independent feature and one dependent feature present.\n",
    "\n",
    "* .Example:\n",
    "\n",
    "suppose if the salary of a person to be predicted on the basis of his years of experience ,  then salary is the\n",
    "dependent variable in this case and the experience is independent variable which is used to predict the dependent variable.\n",
    "    \n",
    "\n",
    "\n",
    "2. Multiple Linear Regression -\n",
    "\n",
    "Multiple Linear Regression is expanding approach of Simple Linear Regression used to predict a single output feature \n",
    "which is dependent by considering more than one independent features to predict it.\n",
    "\n",
    "* .Example:\n",
    "\n",
    "suppose if the price of a house is to be predicted  on the basis of room size , location , number of rooms , number of bathrooms etc. In this case price is dependent feature which is to be predicted by considering all the independent features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828b027-6837-4614-b42c-650bc5cc11da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d435d3-16e1-407c-825b-f48ffbeb8fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f829240-be48-4a3c-b301-2f22876a870a",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d77f8-6d20-4871-be0c-216061332080",
   "metadata": {},
   "source": [
    "Linear regression relies on several assumptions that need to be met for the model to be valid and reliable. These assumptions are crucial to ensure that the estimates and inferences drawn from the regression analysis are accurate. \n",
    "\n",
    "* .Here are the key assumptions of linear regression:\n",
    "\n",
    "1. Linearity: The relationship between the independent variables and the dependent variable should be linear. The model assumes that changes in the dependent variable are linearly associated with changes in the independent variables.\n",
    "\n",
    "2. Independence of Errors (Residuals): The errors (residuals) should be independent of each other. There should be no pattern or correlation in the residuals. Independence ensures that one observation's error does not predict another observation's error.\n",
    "\n",
    "3. Homoscedasticity: Homoscedasticity implies that the variance of residuals should be constant across all levels of the independent variables. In other words, the spread of residuals should remain consistent as the values of the predictors change.\n",
    "\n",
    "4. Normality of Residuals: The residuals should be normally distributed. This assumption ensures that the errors follow a normal distribution with a mean of zero. Normality is crucial for conducting hypothesis tests, constructing confidence intervals, and making statistical inferences.\n",
    "\n",
    "5. No Multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. Multicollinearity can cause issues in estimating the coefficients accurately and can lead to unstable predictions.\n",
    "\n",
    "* .Checking Assumptions:\n",
    "\n",
    "1. Residual Analysis: Plot the residuals against the predicted values. A random scatter around zero in this plot indicates independence and homoscedasticity. Patterns in this plot could signal violations of these assumptions.\n",
    "\n",
    "2. Normality Tests: Use statistical tests like the Shapiro-Wilk test, Q-Q plots, or histogram of residuals to assess normality. These tests determine whether the residuals follow a normal distribution.\n",
    "\n",
    "3. Detecting Multicollinearity: Calculate the variance inflation factor (VIF) for each independent variable. VIF values greater than 5 or 10 suggest multicollinearity.\n",
    "\n",
    "4. Partial Regression Plots: Use partial regression plots to identify whether adding a particular variable significantly improves the model or violates linearity.\n",
    "\n",
    "5. Cook's Distance: Examining Cook's distance helps identify influential data points that might excessively impact the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd217445-b4ee-437f-819b-31cb75223ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097edd7-6985-49d1-a896-9c75801c36a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e922512-dc24-4750-af39-0c5eb8897c9d",
   "metadata": {},
   "source": [
    "## Question - 3\n",
    "ans - \n",
    "\n",
    "\n",
    "In a linear regression model represented by the equation \n",
    "y=mx+c:\n",
    "\n",
    "Slope (m): The slope represents the change in the dependent variable (y) for a one-unit change in the independent variable (x). It indicates the rate of change of y with respect to x. A positive slope indicates a positive relationship, while a negative slope signifies a negative relationship between the variables.\n",
    "\n",
    "Intercept (c): The intercept represents the value of the dependent variable (y) when the independent variable (x) is zero. It is the value of y when \n",
    "\n",
    "x is absent or has no effect.\n",
    "\n",
    "Example:\n",
    "Let's consider a real-world scenario where we want to predict the sales of a product based on the amount spent on advertising.\n",
    "\n",
    "Dependent Variable (Target): Sales (in units)\n",
    "Independent Variable (Predictor): Advertising expenditure (in dollars)\n",
    "Suppose our linear regression model is \n",
    "Sales=50 × Advertising + 100\n",
    "\n",
    "Sales= 50 × Advertising + 100. Here:\n",
    "\n",
    "Slope (m): The slope is 50. It implies that for every additional dollar spent on advertising, the expected increase in sales is 50 units, assuming all other factors remain constant.\n",
    "Intercept (c): The intercept is 100. It means that even if no money is spent on advertising (x=0), the predicted sales would be 100 units.\n",
    "\n",
    "\n",
    "* .Interpreting these coefficients in the context of this example:\n",
    "\n",
    "If $1000 is spent on advertising, the predicted sales would be $50 \\times 1000 + 100 = 51,100$ units.\n",
    "\n",
    "The intercept of 100 indicates that if no money is spent on advertising, the model predicts sales of 100 units.\n",
    "\n",
    "So, the interpretation of slope and intercept in this context helps us understand how changes in advertising \n",
    "\n",
    "expenditure influence sales and what sales value is expected when there is no advertising spending. These coefficients \n",
    "\n",
    "allow us to make predictions and understand the relationship between the variables in the linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c764f2-5e74-4551-a2e1-2501d179a272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36660b7c-3860-47f0-9d6e-50f2ee046ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818c8ad-ccfa-4797-9e7e-793160af2383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed00095e-2083-4b2b-958d-1e0965ec6ee7",
   "metadata": {},
   "source": [
    "## Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910ba2f-461f-43a2-a8ee-3ef4937c3633",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used in machine learning to minimize the cost function of a model by iteratively updating its parameters. It's a fundamental technique used for training models, particularly in scenarios like linear regression, logistic regression, neural networks, and various other machine learning algorithms.\n",
    "\n",
    "Concept of Gradient Descent:\n",
    "Objective: The primary goal of gradient descent is to find the minimum of a cost function \n",
    "\n",
    "(J(θ)). This cost function measures how well the model performs based on the difference between predicted and actual values.\n",
    "\n",
    "* .Gradient Descent Steps:\n",
    "a. Initialization: Start by initializing the parameters (θ) of the model with some arbitrary values.\n",
    "\n",
    "b. Calculate Gradient: Compute the gradient of the cost function with respect to the parameters. The gradient points in the direction of the steepest increase of the function.\n",
    "\n",
    "c. Update Parameters: Adjust the parameters in the opposite direction of the gradient to minimize the cost function. This step involves taking steps proportional to the negative of the gradient, known as the learning rate (α).\n",
    "\n",
    "d. Iterate: Repeat steps b and c until convergence or until a stopping criterion is met (e.g., a maximum number of iterations).\n",
    "\n",
    "* .Learning Rate: The learning rate (α) determines the step size taken in each iteration. A large learning rate may cause overshooting and divergence, while a small learning rate might result in slow convergence.\n",
    "\n",
    "* .Usage in Machine Learning:\n",
    "In machine learning, gradient descent is utilized during the training phase to update the model parameters (weights) iteratively by minimizing the cost function. This process applies to various algorithms such as linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "* .For instance, in linear regression:\n",
    "\n",
    "The cost function to minimize might be the mean squared error (MSE) between predicted and actual values.\n",
    "Gradient descent adjusts the regression coefficients (slope and intercept) to minimize this cost function by updating them based on the gradient of the cost function with respect to these coefficients.\n",
    "\n",
    "\n",
    "* .Types of Gradient Descent:\n",
    "\n",
    "1. Batch Gradient Descent: Computes the gradient using the entire dataset. It can be slow for large datasets due to its computational requirements.\n",
    "\n",
    "2. Stochastic Gradient Descent (SGD): Computes the gradient using a single randomly chosen sample from the dataset at each iteration. Faster than batch gradient descent but can have high variance in the parameter updates.\n",
    "\n",
    "3. Mini-batch Gradient Descent: Computes the gradient using a small batch of samples (between 10 to 1000) at each iteration. It strikes a balance between batch and stochastic gradient descent and is widely used in practice.\n",
    "\n",
    "Gradient Descent, through its various forms, is a fundamental optimization algorithm in machine learning used to iteratively update model parameters to reach the optimal values that minimize the cost function, thereby improving the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1548fd9-cb1e-40ed-b479-c7a56bdde0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281508e-fc85-473b-927f-cd40c0413934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec979f2d-335d-4b16-885c-6e5578344062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7fdce0-70a5-4dda-bc5f-9f83f7375e23",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666a044-2686-4ef9-9a2b-859380211bd6",
   "metadata": {},
   "source": [
    "\n",
    "Multiple linear regression is an extension of simple linear regression that involves multiple independent variables in predicting a single dependent variable. While simple linear regression considers a single predictor, multiple linear regression considers two or more predictors to estimate the relationship between them and the dependent variable.\n",
    "\n",
    "Multiple Linear Regression Model:\n",
    "In multiple linear regression, the relationship between the dependent variable y and n independent variables \n",
    "x1,x2,...,xn is modeled as:\n",
    "\n",
    "\n",
    "y=b0 + b1x1 + b2x2 +...+bnxn + ε\n",
    "\n",
    "Where:\n",
    "y is the dependent variable (target).\n",
    "\n",
    "x1,x2,...,xn are the independent variables (predictors).\n",
    "\n",
    "b0 is the intercept (the value of y when all predictors are zero).\n",
    "\n",
    "b1,b2,...,bn are the coefficients that represent the relationship between each independent variable and the dependent variable.\n",
    "\n",
    "ε represents the error term (residuals), which accounts for the difference between the predicted and actual values not explained by the predictors.\n",
    "\n",
    "* .Differences from Simple Linear Regression:\n",
    "\n",
    "1. Number of Predictors: Multiple linear regression involves more than one predictor variable, while simple linear regression deals with a single predictor.\n",
    "\n",
    "2. Equation Complexity: In simple linear regression, the equation is y=mx+c with only one slope coefficient (m) and an intercept (c). In multiple linear regression, the equation contains multiple predictors, each with its own coefficient.\n",
    "\n",
    "3. Model Complexity and Interpretation: Multiple linear regression accounts for the combined effect of multiple predictors on the dependent variable. It provides a more comprehensive understanding of how each predictor contributes to the outcome, considering interactions among predictors.\n",
    "\n",
    "* . Assumptions and Challenges: Multiple linear regression assumes that the predictors are not highly correlated (multicollinearity), and the model assumes a linear relationship between the predictors and the target variable. Handling multicollinearity and interpreting the impact of multiple predictors can be more complex than in simple linear regression.\n",
    "\n",
    "Example:\n",
    "In a real-world scenario, consider predicting house prices based on various features such as square footage, number of bedrooms, number of bathrooms, and age of the house. Multiple linear regression allows incorporating all these features as predictors to estimate the house price, considering the combined impact of these factors on the target variable.\n",
    "\n",
    "In summary, while simple linear regression deals with a single predictor, multiple linear regression extends the analysis to include multiple predictors, providing a more sophisticated understanding of the relationship between these predictors and the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54543f-0223-43ef-b689-71640fbe35aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd4b00-86df-4ede-ba98-a130d10199f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a45c8b-b7fe-4378-8cce-3a2354d99448",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans - \n",
    "\n",
    "\n",
    "* .Effects of Multicollinearity:\n",
    "\n",
    "1. Unreliable Coefficients: Multicollinearity can make the estimation of regression coefficients unstable or erratic. It becomes difficult to assess the impact of individual predictors on the dependent variable.\n",
    "\n",
    "2. Reduced Model Interpretability: With highly correlated predictors, it's challenging to discern which variable is contributing uniquely to the model, leading to ambiguity in interpretation.\n",
    "\n",
    "* .Detecting Multicollinearity:\n",
    "\n",
    "1. Correlation Matrix: Calculate the correlation coefficients between pairs of predictors. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2. Variance Inflation Factor (VIF): VIF quantifies the severity of multicollinearity by measuring how much the variance of an estimated regression coefficient is increased due to multicollinearity. A VIF value greater than 5 or 10 (some consider lower thresholds) indicates multicollinearity.\n",
    "\n",
    "* .Dealing with Multicollinearity:\n",
    "\n",
    "1. Remove Redundant Predictors: If two predictors are highly correlated, consider removing one of them from the model.\n",
    "\n",
    "2. Feature Selection/Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) or feature selection methods (e.g., Forward Selection, Backward Elimination) can help identify and select a subset of predictors that contribute most to the model while reducing multicollinearity.\n",
    "\n",
    "3. Combine Variables: Instead of using correlated predictors separately, create new variables that combine their information, reducing multicollinearity. For instance, if \"length\" and \"width\" are highly correlated, create a new variable \"area\" by multiplying them.\n",
    "\n",
    "4. Ridge Regression or Lasso Regression: These are regularization techniques that penalize large coefficients, effectively reducing multicollinearity's impact by shrinking coefficients towards zero.\n",
    "\n",
    "5. Centering or Standardizing Variables: Centering variables (subtracting mean) or standardizing them (scaling to have a mean of zero and a standard deviation of one) can sometimes mitigate multicollinearity.\n",
    "\n",
    "6. Collect More Data: Sometimes multicollinearity can be a result of limited data. Gathering more data might help mitigate the issue.\n",
    "\n",
    "It's essential to address multicollinearity to ensure the stability and reliability of the multiple linear regression model. Detecting and mitigating multicollinearity improves the model's interpretability and helps in drawing accurate conclusions about the relationships between predictors and the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e661c7-da18-42aa-a654-59ed3bbe1917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdd7ae-6b9c-41e1-a899-28892c4300fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b3d58b-9c91-4cad-a958-d617dd3fe937",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a5706-adec-4ba1-b4c5-af2b714628e5",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis used when the relationship between the independent variable(s) and the dependent variable is better represented by an nth-degree polynomial equation rather than a linear equation. It extends the concept of linear regression by fitting a curve that is not necessarily a straight line to the data.\n",
    "\n",
    "Polynomial Regression Model:\n",
    "The equation for a polynomial regression model of degree n is:\n",
    "\n",
    "y=b0 + b1x + b2x^2 +...+bnx^n + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "y is the dependent variable.\n",
    "x is the independent variable.\n",
    "b0,b1,...,bn are the coefficients.\n",
    "ε represents the error term.\n",
    "\n",
    "The key difference between polynomial regression and linear regression is that polynomial regression can capture nonlinear relationships between variables by introducing higher-order terms (squared, cubed, etc.) of the predictor variable(s). This allows the model to fit more complex curves to the data.\n",
    "\n",
    "* .Differences from Linear Regression:\n",
    "\n",
    "1. Functional Form: In linear regression, the relationship between variables is linear, represented by a straight line equation (y=mx+c). In polynomial regression, the relationship is described by a polynomial equation of degree n (y=b0+b 1x + b2x^2+...+bnx^n).\n",
    "\n",
    "2. Flexibility: Polynomial regression can capture more complex patterns and nonlinear relationships between variables by incorporating higher-order terms, while linear regression assumes a linear relationship.\n",
    "\n",
    "3. Model Complexity: Polynomial regression models of higher degrees (e.g., cubic, quadratic) are more complex than linear regression models and may be prone to overfitting if not appropriately regularized or constrained.\n",
    "\n",
    "* .Example:\n",
    "\n",
    "Consider a scenario where you are trying to predict the relationship between the temperature (x) and the rate of ice cream sales (y). While linear regression assumes a constant rate of increase in ice cream sales with temperature, polynomial regression can capture a more nuanced relationship, accounting for potential nonlinear changes in sales as temperature varies.\n",
    "\n",
    "For instance, a quadratic polynomial regression model (y=b0 + b1x+ b2x^2) might better fit the data, allowing for curves where sales initially increase with temperature but start to decrease after reaching a certain point.\n",
    "\n",
    "In summary, polynomial regression allows for more flexible modeling of relationships between variables by introducing higher-order terms, enabling the representation of nonlinear patterns that cannot be captured by linear regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622c32b-9b4f-466c-b394-e6699fab8d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8626b-c47c-4f7a-a13d-3ebf184cc1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d73425-96d7-40c9-9ffc-37a4baaced34",
   "metadata": {},
   "source": [
    "## Question - 8\n",
    "ans - \n",
    "\n",
    "--. Advantages of Polynomial Regression over Linear Regression:\n",
    "\n",
    "1. Capturing Nonlinear Relationships: Polynomial regression can capture nonlinear relationships between variables better than linear regression. It can model more complex patterns in the data due to the inclusion of higher-order terms.\n",
    "\n",
    "2. Flexible Fit to Data: It provides a more flexible fit to the data, allowing for curves and bends, which can better represent the underlying relationships between variables.\n",
    "\n",
    "\n",
    "--. Disadvantages of Polynomial Regression:\n",
    "\n",
    "1. Overfitting: Higher-degree polynomial models can lead to overfitting, especially with limited data. They might fit noise in the data, resulting in poor generalization to new, unseen data.\n",
    "\n",
    "2. Increased Complexity: As the degree of the polynomial increases, the model becomes more complex. High-degree polynomials can be challenging to interpret and may lead to computational inefficiency.\n",
    "\n",
    "3. Sensitivity to Outliers: Polynomial regression can be sensitive to outliers, leading to disproportionate effects on the model fit, especially in higher-degree polynomials.\n",
    "\n",
    "\n",
    "***. Situations to Prefer Polynomial Regression:\n",
    "\n",
    "1. Nonlinear Relationships: When the relationship between variables is not linear and shows curvature or bends, polynomial regression can be a suitable choice to capture these complex patterns.\n",
    "\n",
    "2. Flexible Modeling: If the relationship between variables is expected to follow a curve or if the data distribution suggests a nonlinear trend, polynomial regression might be more appropriate.\n",
    "\n",
    "3. Curvilinear Trends: In cases where a linear model doesn't adequately represent the data, and there's a known curvilinear relationship between the variables, polynomial regression can offer a more accurate fit.\n",
    "\n",
    "4. Exploratory Analysis: Polynomial regression can be useful in exploratory analysis to understand the data better, allowing for a more flexible examination of the relationships between variables.\n",
    "\n",
    "In summary, while polynomial regression can be advantageous in capturing nonlinear relationships and providing a flexible fit to the data, it's essential to use it judiciously. One should be cautious about overfitting and consider the trade-off between model complexity and performance. Linear regression might be preferable when the relationship between variables is predominantly linear, simpler models are desired, or when the risk of overfitting is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d341f78-5b7e-4a2b-abbe-03ebfd652ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
